{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63d02832",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NUMERIC_FEATURES' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtabtransformertf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfttransformer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FTTransformerEncoder, FTTransformer\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Encoder is specified separately in case we decide to pre-train the model\u001b[39;00m\n\u001b[0;32m      4\u001b[0m ft_linear_encoder \u001b[38;5;241m=\u001b[39m FTTransformerEncoder(\n\u001b[1;32m----> 5\u001b[0m     numerical_features \u001b[38;5;241m=\u001b[39m \u001b[43mNUMERIC_FEATURES\u001b[49m, \u001b[38;5;66;03m# list of numeric features\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     categorical_features \u001b[38;5;241m=\u001b[39m CATEGORICAL_FEATURES, \u001b[38;5;66;03m# list of categorical features\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     numerical_data \u001b[38;5;241m=\u001b[39m X_train[NUMERIC_FEATURES]\u001b[38;5;241m.\u001b[39mvalues, \u001b[38;5;66;03m# train array of numerical features\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     categorical_data \u001b[38;5;241m=\u001b[39m X_train[CATEGORICAL_FEATURES]\u001b[38;5;241m.\u001b[39mvalues, \u001b[38;5;66;03m# train array of categorical features\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;66;03m# not needed for linear\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     numerical_embedding_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     11\u001b[0m     embedding_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,  \u001b[38;5;66;03m# Embedding dimension (for categorical, numerical, and contextual)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# Number of Transformer Blocks (layers)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m,  \u001b[38;5;66;03m# Number of attention heads in a Transofrmer Block\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     attn_dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,  \u001b[38;5;66;03m# Dropout for attention layers\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     ff_dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,  \u001b[38;5;66;03m# Dropout in Dense layers\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     use_column_embedding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# Fixed column embeddings\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     explainable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# Whether we want to output attention importances or not\u001b[39;00m\n\u001b[0;32m     18\u001b[0m )\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Pass the encoder to the model\u001b[39;00m\n\u001b[0;32m     21\u001b[0m ft_linear_transformer \u001b[38;5;241m=\u001b[39m FTTransformer(\n\u001b[0;32m     22\u001b[0m     encoder\u001b[38;5;241m=\u001b[39mft_linear_encoder,  \u001b[38;5;66;03m# Encoder from above\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     out_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,  \u001b[38;5;66;03m# Number of outputs in final layer\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     out_activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# Activation function for final layer\u001b[39;00m\n\u001b[0;32m     25\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'NUMERIC_FEATURES' is not defined"
     ]
    }
   ],
   "source": [
    "from tabtransformertf.models.fttransformer import FTTransformerEncoder, FTTransformer\n",
    "\n",
    "# Encoder is specified separately in case we decide to pre-train the model\n",
    "ft_linear_encoder = FTTransformerEncoder(\n",
    "    numerical_features = NUMERIC_FEATURES, # list of numeric features\n",
    "    categorical_features = CATEGORICAL_FEATURES, # list of categorical features\n",
    "    numerical_data = X_train[NUMERIC_FEATURES].values, # train array of numerical features\n",
    "    categorical_data = X_train[CATEGORICAL_FEATURES].values, # train array of categorical features\n",
    "    y = None, # not needed for linear\n",
    "    numerical_embedding_type='linear',\n",
    "    embedding_dim=16,  # Embedding dimension (for categorical, numerical, and contextual)\n",
    "    depth=3,  # Number of Transformer Blocks (layers)\n",
    "    heads=6,  # Number of attention heads in a Transofrmer Block\n",
    "    attn_dropout=0.2,  # Dropout for attention layers\n",
    "    ff_dropout=0.2,  # Dropout in Dense layers\n",
    "    use_column_embedding=True,  # Fixed column embeddings\n",
    "    explainable=True  # Whether we want to output attention importances or not\n",
    ")\n",
    "\n",
    "# Pass the encoder to the model\n",
    "ft_linear_transformer = FTTransformer(\n",
    "    encoder=ft_linear_encoder,  # Encoder from above\n",
    "    out_dim=1,  # Number of outputs in final layer\n",
    "    out_activation='sigmoid',  # Activation function for final layer\n",
    ")\n",
    "\n",
    "preds = ft_linear_transformer.predict(train_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLDA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
